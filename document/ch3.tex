\chapter{Моделювання}


Для моделювання були розглянуті застосовані лінійна регресія та нейронна мережа. 
Кожна модель має свої переваги і свої недоліки. 
Умови, основи математичного обгрунтування, код, переваги та недоліки кожної моделі будуть розглянуті у відповідних підрозділах данного розділу і будуть підбиті підсумки у останньому підрозділі.

Дані, використані для отримання коефіцієнтів або навчання моделі, охоплюють період з 2021 по 2023 роки. 
Період прогнозування складає перші три місяці 2024 року. 
Для більш детального аналізу буде проведено порівняння точності короткострокових прогнозів з довгостроковими. 
Це дозволить оцінити ефективність моделі на різних часових проміжках, порівнюючи результати прогнозування для перших тижнів 2024 року з результатами для останніх тижнів цього ж періоду.

\section{Лінійна регресія}

\subsection{Опис моделі}

Лінійна регресія це модель, де прогнозована величина наближається за допомогою лінійної комбінації змінних:

$$y_i = \beta_0 + \beta_1 x_{i 1} + \dots + \beta_p x_{i p} + \varepsilon_i$$

де $y_i$ прогнозована величина за $i$-ий проміжок часу, $x_j$ змінні, за якими відбувається прогнозування, $\beta_j$ коефіцієнти, які визначають лінійну залежність, $\varepsilon_i$ помилка моделі. 
Для лінійної регресії з багатьма параметрами частіше використовують матричний запис:

$$Y = X\beta + \varepsilon$$

$$
Y = \begin{pmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_n
    \end{pmatrix}, 
X = \begin{pmatrix}
        1 & x_{1 1} & \dots & x_{1 p}\\
        1 & x_{2 1} & \dots & x_{2 p}\\
        \vdots & \vdots & \ddots & \vdots \\
        1 & x_{n 1} & \dots & x_{n p}
    \end{pmatrix}, 
\beta = \begin{pmatrix}
            \beta_0 \\
            \beta_1 \\ 
            \vdots \\
            \beta_p
        \end{pmatrix}, 
\varepsilon =   \begin{pmatrix}
                    \varepsilon_1 \\
                    \varepsilon_2 \\
                    \vdots \\
                    \varepsilon_n 
                \end{pmatrix}
$$

Коефіцієнти вектора $\beta$ знаходяться за допомогою методу найменших квадратів тобто обераються такі значення, за яких набуває найменшого значення вираз: 

$$ S(\beta) = \sum_{i = 1}^{n} ( y_i - \sum_{j = 1}^{p} X_{ij} \beta_j )^2 = |\| y - X\beta \|^2 $$

З припущенням, що всі стовпці матриці $X$ незалежні (не має залежних параметрів моделі) метод має єдиний розв'язок:

$$\hat{\beta} = (X^T X)^{-1} X^T Y$$

Для застосування подібних моделей були створені припущення, яких варто притримуватись задля коректного застосування тої чи іншої моделі:

\begin{itemize}
    \item Слабка екзогенність. Незважаючи на випадкову природу більшості життєвих процесів до, яких застосовується регресія, вхідні дані розглядаються як фіксовані значення тобто припускається, що дані не містять помилок у вимірюванні

    \item Лінійність. З визначення моделі слідує, що середнє значення відповіді є лінійною комбінацією параметрів. На перший погляд це досить сильно обмежує можливості моделі, проте це припущення не обмежує перетворення оригінальних даних до параметрів, тобто пред застосуванням регресії не рідко дані логарифмують і нормують, але коефіцієнти регресії $\beta$ залишаються лінійними

    \item  Постійна дисперсія. Для великих і малих величин похибки має лишатись однаковою.
\end{itemize}

Порушення цих припущень призводить до упереджених оцінок коефіцієнтів, ненадійних довірчих інтервалів і тестів на значимість.

Перші два припущення ми можемо лише перевірити постфактум, на відміну від останнього. 
Як вже було описано в минулому розділі лише розподіл викидів озону схожий на нормальний, проте після логарифмічного перетворення дані стосовно твердих часток і вуглекислого газу можна використати для застосування лінійної регресії.

\subsection{Результати}

\section{Нейронна мережа}

\subsection{Опис моделі}

Нейронна мережа це модель, що складається з нейронів, які з'єднані між собою. 
Кожен нейрон приймає певну кількість параметрів, однакову для всіх нейронів з одного шару, і видає вихідне значення. 
Вихідне значення обчислюється, як скалярний добуток параметрів певного нейрона на вхідні значення, після чого використовується активаційна функція. 

\begin{figure}[H]
    \begin{center}
        \scalebox{0.8}{
            \begin{tikzpicture}[
                roundnode/.style={circle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
                roundnode1/.style={circle, draw=blue!60, fill=blue!5, very thick, minimum size=20mm, scale= 1},
                roundnode2/.style={circle, draw=blue!60, very thick, minimum size=10mm, scale= 2},
                dot/.style={circle, fill=black, ultra thin, scale=0.5},
                ]
                %Nodes
                \node[roundnode]    (x1)                 {$x_1$};
                \node[roundnode]    (x2)   [below=of x1] {$x_2$};
                
                \node[dot]          (d1)   [below=of x2] {};
                \node[dot]          (d2)   [below=of d1] {};
                \node[dot]          (d3)   [below=of d2] {};
                
                \node[roundnode]    (xn)   [below=of d3] {$x_n$};
                
                
                
                \node[roundnode1]    (n1)   [right=of d1, xshift= 1.5cm] {$\Sigma$};
                
                \node[roundnode1]    (n2)   [right=of n1] {$g(*)$};
                
                \node[roundnode2]    (n3)   [right=of n2] {$y$};
                
                %Lines
                \draw[-biggertip] (x1.east) -- (n1.west);
                \draw[-biggertip] (x2.east) -- (n1.west);
                
                \draw[-biggertip] (xn.east) -- (n1.west);

                \draw[-biggertip] (n1.east) -- (n2.west);
                
                \draw[-biggertip] (n2.east) -- (n3.west);
                
            \end{tikzpicture}
            }
            \caption{Схема нейрона}
    \end{center}
\end{figure}

На рис. 3.1 $x_1, x_2, \dots x_n$ - вхідні значення, $\Sigma$ - скалярний добуток $(X, w)$ де $w$ параметри нейрона, $g$ - активаційна функція, $y$ - вихідне значення. 

\begin{figure}[H]
    \begin{center}
        \scalebox{0.8}{
            \begin{tikzpicture}[
                roundnode/.style={circle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
                roundnode1/.style={circle, draw=blue!60, fill=blue!5, very thick, minimum size=20mm, scale= 1},
                roundnode2/.style={circle, draw=blue!60, very thick, minimum size=20mm, scale= 1},
                dot/.style={circle, fill=black, ultra thin, scale=0.5},
                ]
                %Nodes
                \node[roundnode]    (x1)                 {$x_1$};
                \node[roundnode]    (x2)   [below=of x1] {$x_2$};
                
                \node[dot]          (d1)   [below=of x2] {};
                \node[dot]          (d2)   [below=of d1] {};
                \node[dot]          (d3)   [below=of d2] {};
                
                \node[roundnode]    (xn)   [below=of d3] {$x_n$};
                
                
                
                \node[roundnode1]    (n1)   [right=of x1, xshift= 1.5cm, yshift = -1cm] {$n_1$};
                \node[roundnode1]    (n2)   [below=of n1] {$n_2$};
                \node[roundnode1]    (n3)   [below=of n2] {$n_3$};
                
                \node[roundnode1]    (n5)   [right=of n2, yshift= - 1.75cm] {$n_5$};
                \node[roundnode1]    (n4)   [above=of n5] {$n_4$};

                \node[roundnode1]    (n6)   [right=of n4, yshift= - 1.7cm] {$n_6$};

                \node[roundnode2]    (n7)   [right=of n6] {$y$};
            
                %Lines
                \draw[-biggertip] (x1.east) -- (n1.west);
                \draw[-biggertip] (x2.east) -- (n1.west);
                \draw[-biggertip] (xn.east) -- (n1.west);

                \draw[-biggertip] (x1.east) -- (n2.west);
                \draw[-biggertip] (x2.east) -- (n2.west);
                \draw[-biggertip] (xn.east) -- (n2.west);

                \draw[-biggertip] (x1.east) -- (n3.west);
                \draw[-biggertip] (x2.east) -- (n3.west);
                \draw[-biggertip] (xn.east) -- (n3.west);


                \draw[-biggertip] (n1.east) -- (n4.west);
                \draw[-biggertip] (n2.east) -- (n4.west);
                \draw[-biggertip] (n3.east) -- (n4.west);

                \draw[-biggertip] (n1.east) -- (n5.west);
                \draw[-biggertip] (n2.east) -- (n5.west);
                \draw[-biggertip] (n3.east) -- (n5.west);
                

                \draw[-biggertip] (n4.east) -- (n6.west);
                \draw[-biggertip] (n5.east) -- (n6.west);


                \draw[-biggertip] (n6.east) -- (n7.west);


            \end{tikzpicture}
            }
            \caption{Схема нейрона}
    \end{center}
\end{figure}




    
    
\subsection{Результати}
