\chapter{Моделювання}


Для моделювання були розглянуті застосовані лінійна регресія та нейронна мережа. 
Кожна модель має свої переваги і свої недоліки. 
Умови, основи математичного обгрунтування, код, переваги та недоліки кожної моделі будуть розглянуті у відповідних підрозділах данного розділу і будуть підбиті підсумки у останньому підрозділі.

Дані, використані для отримання коефіцієнтів або навчання моделі, охоплюють період з 2021 по 2023 роки. 
Період прогнозування складає перші три місяці 2024 року. 
Для більш детального аналізу буде проведено порівняння точності короткострокових прогнозів з довгостроковими. 
Це дозволить оцінити ефективність моделі на різних часових проміжках, порівнюючи результати прогнозування для перших тижнів 2024 року з результатами для останніх тижнів цього ж періоду.

\section{Лінійна регресія}

\subsection{Опис моделі}

Лінійна регресія це модель, де прогнозована величина наближається за допомогою лінійної комбінації змінних:

$$y_i = \beta_0 + \beta_1 x_{i 1} + \dots + \beta_p x_{i p} + \varepsilon_i$$

де $y_i$ прогнозована величина за $i$-ий проміжок часу, $x_j$ змінні, за якими відбувається прогнозування, $\beta_j$ коефіцієнти, які визначають лінійну залежність, $\varepsilon_i$ помилка моделі. 
Для лінійної регресії з багатьма параметрами частіше використовують матричний запис:

$$Y = X\beta + \varepsilon$$

$$
Y = \begin{pmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_n
    \end{pmatrix}, 
X = \begin{pmatrix}
        1 & x_{1 1} & \dots & x_{1 p}\\
        1 & x_{2 1} & \dots & x_{2 p}\\
        \vdots & \vdots & \ddots & \vdots \\
        1 & x_{n 1} & \dots & x_{n p}
    \end{pmatrix}, 
\beta = \begin{pmatrix}
            \beta_0 \\
            \beta_1 \\ 
            \vdots \\
            \beta_p
        \end{pmatrix}, 
\varepsilon =   \begin{pmatrix}
                    \varepsilon_1 \\
                    \varepsilon_2 \\
                    \vdots \\
                    \varepsilon_n 
                \end{pmatrix}
$$

Коефіцієнти вектора $\beta$ знаходяться за допомогою методу найменших квадратів тобто обераються такі значення, за яких набуває найменшого значення вираз: 

$$ S(\beta) = \sum_{i = 1}^{n} ( y_i - \sum_{j = 1}^{p} X_{ij} \beta_j )^2 = |\| y - X\beta \|^2 $$

З припущенням, що всі стовпці матриці $X$ незалежні (не має залежних параметрів моделі) метод має єдиний розв'язок:

$$\hat{\beta} = (X^T X)^{-1} X^T Y$$

Для застосування подібних моделей були створені припущення, яких варто притримуватись задля коректного застосування тої чи іншої моделі:

\begin{itemize}
    \item Слабка екзогенність. Незважаючи на випадкову природу більшості життєвих процесів до, яких застосовується регресія, вхідні дані розглядаються як фіксовані значення тобто припускається, що дані не містять помилок у вимірюванні

    \item Лінійність. З визначення моделі слідує, що середнє значення відповіді є лінійною комбінацією параметрів. На перший погляд це досить сильно обмежує можливості моделі, проте це припущення не обмежує перетворення оригінальних даних до параметрів, тобто пред застосуванням регресії не рідко дані логарифмують і нормують, але коефіцієнти регресії $\beta$ залишаються лінійними

    \item  Постійна дисперсія. Для великих і малих величин похибки має лишатись однаковою.
\end{itemize}

Порушення цих припущень призводить до упереджених оцінок коефіцієнтів, ненадійних довірчих інтервалів і тестів на значимість.

Перші два припущення ми можемо лише перевірити постфактум, на відміну від останнього. 
Як вже було описано в минулому розділі лише розподіл викидів озону схожий на нормальний, проте після логарифмічного перетворення дані стосовно твердих часток і вуглекислого газу можна використати для застосування лінійної регресії.

\subsection{Результати}

\section{Кластеризація та регресія}


\section{Нейронна мережа}



\subsection{Опис моделі}

\subsection{Результати}
