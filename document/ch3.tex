\chapter{Моделювання}


Для моделювання були розглянуті застосовані лінійна регресія та нейронна мережа. 
Кожна модель має свої переваги і свої недоліки. 
Умови, основи математичного обгрунтування, код, переваги та недоліки кожної моделі будуть розглянуті у відповідних підрозділах данного розділу і будуть підбиті підсумки у останньому підрозділі.

Дані, використані для отримання коефіцієнтів або навчання моделі, охоплюють період з 2021 по 2023 роки. 
Період прогнозування складає перші три місяці 2024 року. 
Для більш детального аналізу буде проведено порівняння точності короткострокових прогнозів з довгостроковими. 
Це дозволить оцінити ефективність моделі на різних часових проміжках, порівнюючи результати прогнозування для перших тижнів 2024 року з результатами для останніх тижнів цього ж періоду.

\section{Лінійна регресія}

\subsection{Опис моделі}

Лінійна регресія це модель, де прогнозована величина наближається за допомогою лінійної комбінації змінних:

$$y_i = \beta_0 + \beta_1 x_{i 1} + \dots + \beta_p x_{i p} + \varepsilon_i$$

де $y_i$ прогнозована величина за $i$-ий проміжок часу, $x_j$ змінні, за якими відбувається прогнозування, $\beta_j$ коефіцієнти, які визначають лінійну залежність, $\varepsilon_i$ помилка моделі. 
Для лінійної регресії з багатьма параметрами частіше використовують матричний запис:

$$Y = X\beta + \varepsilon$$

$$
Y = \begin{pmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_n
    \end{pmatrix}, 
X = \begin{pmatrix}
        1 & x_{1 1} & \dots & x_{1 p}\\
        1 & x_{2 1} & \dots & x_{2 p}\\
        \vdots & \vdots & \ddots & \vdots \\
        1 & x_{n 1} & \dots & x_{n p}
    \end{pmatrix}, 
\beta = \begin{pmatrix}
            \beta_0 \\
            \beta_1 \\ 
            \vdots \\
            \beta_p
        \end{pmatrix}, 
\varepsilon =   \begin{pmatrix}
                    \varepsilon_1 \\
                    \varepsilon_2 \\
                    \vdots \\
                    \varepsilon_n 
                \end{pmatrix}
$$

Коефіцієнти вектора $\beta$ знаходяться за допомогою методу найменших квадратів тобто обераються такі значення, за яких набуває найменшого значення вираз: 

$$ S(\beta) = \sum_{i = 1}^{n} ( y_i - \sum_{j = 1}^{p} X_{ij} \beta_j )^2 = |\| y - X\beta \|^2 $$

З припущенням, що всі стовпці матриці $X$ незалежні (не має залежних параметрів моделі) метод має єдиний розв'язок:

$$\hat{\beta} = (X^T X)^{-1} X^T Y$$

Для застосування подібних моделей були створені припущення, яких варто притримуватись задля коректного застосування тої чи іншої моделі:

\begin{itemize}
    \item Слабка екзогенність. Незважаючи на випадкову природу більшості життєвих процесів до, яких застосовується регресія, вхідні дані розглядаються як фіксовані значення тобто припускається, що дані не містять помилок у вимірюванні

    \item Лінійність. З визначення моделі слідує, що середнє значення відповіді є лінійною комбінацією параметрів. На перший погляд це досить сильно обмежує можливості моделі, проте це припущення не обмежує перетворення оригінальних даних до параметрів, тобто пред застосуванням регресії не рідко дані логарифмують і нормують, але коефіцієнти регресії $\beta$ залишаються лінійними

    \item  Постійна дисперсія. Для великих і малих величин похибки має лишатись однаковою.
\end{itemize}

Порушення цих припущень призводить до упереджених оцінок коефіцієнтів, ненадійних довірчих інтервалів і тестів на значимість.

Перші два припущення ми можемо лише перевірити постфактум, на відміну від останнього. 
Як вже було описано в минулому розділі лише розподіл викидів озону схожий на нормальний, проте після логарифмічного перетворення дані стосовно твердих часток і вуглекислого газу можна використати для застосування лінійної регресії.

\subsection{Результати}

\section{Нейронна мережа}

\subsection{Опис моделі}

Нейронна мережа це модель, що складається з штучних нейронів, які з'єднані між собою. 
Концепція була створена на основі біологічних нейронних мереж, які складаються з нейронів, поведінка і структура яких не до кінця вивчена. 
Штучні нейрони мають більш просту структуру - кожен приймає певну кількість параметрів, однакову для всіх нейронів з одного шару, і видає вихідне значення. 
Вихідне значення обчислюється, як скалярний добуток параметрів певного нейрона на вхідні значення, після чого використовується активаційна функція. 
Параметри нейронів змінюються під час навчання моделі. 

\begin{figure}[H]
    \begin{center}
        \scalebox{0.8}{
            \begin{tikzpicture}[
                roundnode/.style={circle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
                roundnode1/.style={circle, draw=blue!60, fill=blue!5, very thick, minimum size=20mm, scale= 1},
                roundnode2/.style={circle, draw=white, very thick, minimum size=10mm, scale= 2},
                dot/.style={circle, fill=black, ultra thin, scale=0.5},
                ]
                %Nodes
                \node[roundnode]    (x1)                 {$x_1$};
                \node[roundnode]    (x2)   [below=of x1] {$x_2$};
                
                \node[dot]          (d1)   [below=of x2] {};
                \node[dot]          (d2)   [below=of d1] {};
                \node[dot]          (d3)   [below=of d2] {};
                
                \node[roundnode]    (xn)   [below=of d3] {$x_n$};
                
                
                
                \node[roundnode1]    (n1)   [right=of d1, xshift= 1.5cm] {$\Sigma$};

                \node[roundnode2]    (n4)   [above=of n1] {$w$};
                
                \node[roundnode1]    (n2)   [right=of n1] {$g(*)$};
                
                \node[roundnode2]    (n3)   [right=of n2] {$y$};
                
                %Lines
                \draw[-biggertip] (x1.east) -- (n1.west);
                \draw[-biggertip] (x2.east) -- (n1.west);
                
                \draw[-biggertip] (xn.east) -- (n1.west);

                \draw[-biggertip] (n1.east) -- (n2.west);
                
                \draw[-biggertip] (n2.east) -- (n3.west);

                \draw[-biggertip] (n4.south) -- (n1.north);
                
            \end{tikzpicture}
            }
            \caption{Схема нейрона}
    \end{center}
\end{figure}

На рис. 3.1 $x_1, x_2, \dots x_n$ - вхідні значення, $\Sigma$ - скалярний добуток $(X, w)$ де $w$ параметри нейрона, $g$ - активаційна функція, $y$ - вихідне значення. 

Найпоширеніші функції акивації:

\begin{itemize}
    \item Сигмоїда: $g(x) = \frac{1}{1 + e^{-x}}$
    \item Relu: $g(x) = \max(0, x)$
    \item Гіперболічний тангенс: $g(x) = \tanh(x)$
    \item Softmax: $g(x) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$
\end{itemize}

Нейронні мережі зазвичай складаються з багатьох шарів. 
Перший шар приймає як вхідні дані параметри моделі, а останній видає прогнозоване значення. 
Нейрони на проміжних шарах приймають, як вхідні значення вихідні значення нейронів з попереднього шару. 
Для останнього нейрона часто використовують relu або softmax як функцію активації для задач класифікації одного класа або багатьох відповідно.


\begin{figure}[H]
    \begin{center}
        \scalebox{0.8}{
            \begin{tikzpicture}[
                roundnode/.style={circle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
                roundnode1/.style={circle, draw=blue!60, fill=blue!5, very thick, minimum size=20mm, scale= 1},
                roundnode2/.style={circle, draw=blue!60, very thick, minimum size=20mm, scale= 1},
                dot/.style={circle, fill=black, ultra thin, scale=0.5},
                ]
                %Nodes
                \node[roundnode]    (x1)                 {$x_1$};
                \node[roundnode]    (x2)   [below=of x1] {$x_2$};
                
                \node[dot]          (d1)   [below=of x2] {};
                \node[dot]          (d2)   [below=of d1] {};
                \node[dot]          (d3)   [below=of d2] {};
                
                \node[roundnode]    (xn)   [below=of d3] {$x_n$};
                
                
                
                \node[roundnode1]    (n1)   [right=of x1, xshift= 1.5cm, yshift = -1cm] {$n_1$};
                \node[roundnode1]    (n2)   [below=of n1] {$n_2$};
                \node[roundnode1]    (n3)   [below=of n2] {$n_3$};
                
                \node[roundnode1]    (n5)   [right=of n2, yshift= - 1.75cm] {$n_5$};
                \node[roundnode1]    (n4)   [above=of n5] {$n_4$};

                \node[roundnode1]    (n6)   [right=of n4, yshift= - 1.7cm] {$n_6$};

                \node[roundnode2]    (n7)   [right=of n6] {$y$};
            
                %Lines
                \draw[-biggertip] (x1.east) -- (n1.west);
                \draw[-biggertip] (x2.east) -- (n1.west);
                \draw[-biggertip] (xn.east) -- (n1.west);

                \draw[-biggertip] (x1.east) -- (n2.west);
                \draw[-biggertip] (x2.east) -- (n2.west);
                \draw[-biggertip] (xn.east) -- (n2.west);

                \draw[-biggertip] (x1.east) -- (n3.west);
                \draw[-biggertip] (x2.east) -- (n3.west);
                \draw[-biggertip] (xn.east) -- (n3.west);


                \draw[-biggertip] (n1.east) -- (n4.west);
                \draw[-biggertip] (n2.east) -- (n4.west);
                \draw[-biggertip] (n3.east) -- (n4.west);

                \draw[-biggertip] (n1.east) -- (n5.west);
                \draw[-biggertip] (n2.east) -- (n5.west);
                \draw[-biggertip] (n3.east) -- (n5.west);
                

                \draw[-biggertip] (n4.east) -- (n6.west);
                \draw[-biggertip] (n5.east) -- (n6.west);


                \draw[-biggertip] (n6.east) -- (n7.west);


            \end{tikzpicture}
            }
            \caption{Схема нейронної мережі з трьома шарами}
    \end{center}
\end{figure}

Перед початком навчання ваги та зсуви ініціалізуються випадковими значеннями. 
Це допомагає уникнути симетрії між нейронами і забезпечує різні шляхи навчання для кожного з них. 
Далі йде розрахунок кінцевого значення моделі, яке порівнюється з реальним значенням за допомогою функції втрат. 
Функції витрат визначають, наскільки відрізняються прогнозовані значення від реальних. 
Після цього відбувається корекції ваг та зсувів. 
Якщо розгладати функцію витрат, як багатовимірну функцію від ваг, то корекція полягає у застосування градієнтного методу оптимізації. 
Градієнт розграховується в оберненому порідку від обчислення результату моделі, тобто спочатку оптимізуються ваги останнього шару, потім передостаннього і так далі. 
Функції активації підібрані так, що градієнт можна обчислити аналітично. 
Цей процес повторюється для кожного набору даних, поки результат не задовільнить визначені критерії визначені для моделі. 

Для нашого випадку велика нейронна мережа з багатьма шарами не підійде, адже під час навчання мадель запам'ятає дані і не зважаючі на те, що результат на навчальних даних буде дуже точним, на тестових даних результат буде поганим. 
У ході експериментів з різними розмірами моделі було виявлено, що найкращі результати показує модель з трьому шарами. 


%схема моделі 


\subsection{Результати}
