\chapter{Моделювання}


Для моделювання були розглянуті застосовані лінійна регресія та нейронна мережа. 
Кожна модель має свої переваги і свої недоліки. 
Умови, основи математичного обгрунтування, код, переваги та недоліки кожної моделі будуть розглянуті у відповідних підрозділах данного розділу і будуть підбиті підсумки у останньому підрозділі.

Дані, використані для отримання коефіцієнтів або навчання моделі, охоплюють період з 2021 по 2023 роки. 
Період прогнозування складає перші три місяці 2024 року. 
Для більш детального аналізу буде проведено порівняння точності короткострокових прогнозів з довгостроковими. 
Це дозволить оцінити ефективність моделі на різних часових проміжках, порівнюючи результати прогнозування для перших тижнів 2024 року з результатами для останніх тижнів цього ж періоду.

\section{Лінійна регресія}

\subsection{Опис моделі}

Лінійна регресія - це модель, де прогнозована величина наближається за допомогою лінійної комбінації змінних:

$$y_i = \beta_0 + \beta_1 x_{i 1} + \dots + \beta_p x_{i p} + \varepsilon_i$$

де $y_i$ - прогнозована величина за $i$-ий проміжок часу, $x_j$ - змінні, за якими відбувається прогнозування, $\beta_j$ - коефіцієнти, які визначають лінійну залежність, $\varepsilon_i$ - помилка моделі.\cite{linear_reg} 
Для лінійної регресії з багатьма параметрами частіше використовують матричний запис:

$$Y = X\beta + \varepsilon$$

$$
Y = \begin{pmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_n
    \end{pmatrix}, 
X = \begin{pmatrix}
        1 & x_{1 1} & \dots & x_{1 p}\\
        1 & x_{2 1} & \dots & x_{2 p}\\
        \vdots & \vdots & \ddots & \vdots \\
        1 & x_{n 1} & \dots & x_{n p}
    \end{pmatrix}, 
\beta = \begin{pmatrix}
            \beta_0 \\
            \beta_1 \\ 
            \vdots \\
            \beta_p
        \end{pmatrix}, 
\varepsilon =   \begin{pmatrix}
                    \varepsilon_1 \\
                    \varepsilon_2 \\
                    \vdots \\
                    \varepsilon_n 
                \end{pmatrix}
$$

Коефіцієнти вектора $\beta$ знаходяться за допомогою методу найменших квадратів тобто обираються такі значення, за яких набуває найменшого значення вираз: 

$$ S(\beta) = \sum_{i = 1}^{n} ( y_i - \sum_{j = 1}^{p} X_{ij} \beta_j )^2 = |\| y - X\beta \|^2 $$

З припущенням, що всі стовпці матриці $X$ незалежні (немає залежних параметрів моделі) метод має єдиний розв'язок:

$$\hat{\beta} = (X^T X)^{-1} X^T Y$$

Для застосування подібних моделей були створені припущення, яких варто притримуватись задля коректного застосування тої чи іншої моделі:

\begin{itemize}
    \item Слабка екзогенність. Незважаючи на випадкову природу більшості життєвих процесів до яких застосовується регресія, вхідні дані розглядаються як фіксовані значення тобто припускається, що дані не містять помилок у вимірюванні,

    \item Лінійність. З визначення моделі слідує, що середнє значення відповіді є лінійною комбінацією параметрів. На перший погляд, це досить сильно обмежує можливості моделі, проте це припущення не обмежує перетворення оригінальних даних до параметрів, тобто перед застосуванням регресії часто дані логарифмують і нормують, але коефіцієнти регресії $\beta$ залишаються лінійними,

    \item  Постійна дисперсія. Для великих і малих величин похибка має лишатись однаковою.
\end{itemize}

Порушення цих припущень призводить до упереджених оцінок коефіцієнтів, ненадійних довірчих інтервалів і тестів на значимість.

Перші два припущення ми можемо лише перевірити постфактум, на відміну від останнього.\cite{linear_reg2} 
Як вже було описано в минулому розділі лише розподіл викидів озону схожий на нормальний, проте після логарифмічного перетворення дані стосовно твердих часток і вуглекислого газу можна використати для застосування лінійної регресії.

\subsection{Результати}

Лінійну регресію було застосовано до даних про викиди вуглекислого газу, озону і твердих часток. 
Лише розподіли цих даних мали розподіл подібний до нормального. 


\begin{center}
    \scalebox{1}{
        \begin{tabular}{ |p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
            \hline
            \multicolumn{3}{|c|}{Середня похибка} \\
            \hline
            Викиди & регресія & наївний алг  \\
            \hline
            \multicolumn{3}{|c|}{"Тренувальні" дані} \\
            \hline
            CO2  &   5.13 \% &   6.29 \% \\
            \hline
            pm10  &  10.70 \% &  15.08 \% \\
            \hline
            pm2.5  &  12.12 \% &  17.99 \% \\
            \hline
            озон  &   2.94 \% &   3.72 \% \\
            \hline
            \multicolumn{3}{|c|}{Тестові дані} \\
            \hline
            CO2  &  16.52 \% &  20.04 \% \\
            \hline
            pm10  &  18.78 \% &  19.76 \% \\
            \hline
            pm2.5  &  12.50 \% &  14.55 \% \\
            \hline
            озон  &   8.27 \% &  12.04 \% \\
            \hline
        \end{tabular}
    }
    \vspace{1cm}

    \labelformat{2}{}{Таблиця 3.1: Результат лінійної регресії в порівнянні з наївним алгоритмом}
\end{center}

Для кращого розуміння результатів роботи моделі було проведено порівняння з наївним алгоритмом, який використовує значення попереднього дня для прогнозування. 
Цей алгоритм не потребує жодних параметрів і обчислень, тому порівняння нашої моделі з ним дозволяє оцінити, чи враховані моделю дані мають вищу залежність від неврахованих параметрів.

Результати роботи лінійної регресії на тренувальних даних показують, що модель має помірну точність для прогнозування рівня вуглекислого газу та озону, про що свідчать відносно низькі середні похибки. 
Для PM10 та PM2.5 середні похибки вищі, що свідчить про те, що модель менш ефективно справляється з прогнозуванням цих параметрів на тренувальних даних.

На тестових даних модель також показує, що дані переважно залежать від параметрів. 
Наприклад, для вуглекислого газу наша модель має помітно нижчу похибку порівняно з наївним алгоритмом, що свідчить про кращу здатність моделі враховувати залежності в даних. 
Однак для PM10 і PM2.5 результати показують, що наша модель може незначно поступатися наївному алгоритму в деяких випадках, що вказує на необхідність вдосконалення моделі шляхом пошуку параметрів, які впливають на ці викиди. 
Можливо, дані містять випадкові чинники, створені неточностями вимірювань. 
Для озону наша модель демонструє найкращу точність, що може бути пов’язано з розподілом даних, найбільш подібним до нормального.

\section{Нейронна мережа}

\subsection{Опис моделі}

Нейронна мережа це модель, що складається з штучних нейронів, які з'єднані між собою. 
Концепція була створена на основі біологічних нейронних мереж, які складаються з нейронів, поведінка і структура яких не до кінця вивчена. 
Штучні нейрони мають більш просту структуру - кожен приймає певну кількість параметрів, однакову для всіх нейронів з одного шару, і видає вихідне значення. 
Вихідне значення обчислюється, як скалярний добуток параметрів певного нейрона на вхідні значення, після чого використовується активаційна функція. 
Параметри нейронів змінюються під час навчання моделі. 

\begin{figure}[H]
    \begin{center}
        \scalebox{0.8}{
            \begin{tikzpicture}[
                roundnode/.style={circle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
                roundnode1/.style={circle, draw=blue!60, fill=blue!5, very thick, minimum size=20mm, scale= 1},
                roundnode2/.style={circle, draw=white, very thick, minimum size=10mm, scale= 2},
                dot/.style={circle, fill=black, ultra thin, scale=0.5},
                ]
                %Nodes
                \node[roundnode]    (x1)                 {$x_1$};
                \node[roundnode]    (x2)   [below=of x1] {$x_2$};
                
                \node[dot]          (d1)   [below=of x2] {};
                \node[dot]          (d2)   [below=of d1] {};
                \node[dot]          (d3)   [below=of d2] {};
                
                \node[roundnode]    (xn)   [below=of d3] {$x_n$};
                
                
                
                \node[roundnode1]    (n1)   [right=of d1, xshift= 1.5cm] {$\Sigma$};

                \node[roundnode2]    (n4)   [above=of n1] {$w$};
                
                \node[roundnode1]    (n2)   [right=of n1] {$g(*)$};
                
                \node[roundnode2]    (n3)   [right=of n2] {$y$};
                
                %Lines
                \draw[-biggertip] (x1.east) -- (n1.west);
                \draw[-biggertip] (x2.east) -- (n1.west);
                
                \draw[-biggertip] (xn.east) -- (n1.west);

                \draw[-biggertip] (n1.east) -- (n2.west);
                
                \draw[-biggertip] (n2.east) -- (n3.west);

                \draw[-biggertip] (n4.south) -- (n1.north);
                
            \end{tikzpicture}
            }
            \caption{Схема нейрона}
    \end{center}
\end{figure}

На рис. 3.1 $x_1, x_2, \dots x_n$ - вхідні значення, $\Sigma$ - скалярний добуток $(X, w)$ де $w$ параметри нейрона, $g$ - активаційна функція, $y$ - вихідне значення. 

Найпоширеніші функції акивації:

\begin{itemize}
    \item Сигмоїда: $g(x) = \frac{1}{1 + e^{-x}}$
    \item Relu: $g(x) = \max(0, x)$
    \item Гіперболічний тангенс: $g(x) = \tanh(x)$
    \item Softmax: $g(x) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$
\end{itemize}

Нейронні мережі зазвичай складаються з багатьох шарів. 
Перший шар приймає як вхідні дані параметри моделі, а останній видає прогнозоване значення. 
Нейрони на проміжних шарах приймають, як вхідні значення вихідні значення нейронів з попереднього шару. 
Для останнього нейрона часто використовують relu або softmax як функцію активації для задач класифікації одного класа або багатьох відповідно.\cite{neural}


\begin{figure}[H]
    \begin{center}
        \scalebox{0.8}{
            \begin{tikzpicture}[
                roundnode/.style={circle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
                roundnode1/.style={circle, draw=blue!60, fill=blue!5, very thick, minimum size=20mm, scale= 1},
                roundnode2/.style={circle, draw=blue!60, very thick, minimum size=20mm, scale= 1},
                dot/.style={circle, fill=black, ultra thin, scale=0.5},
                ]
                %Nodes
                \node[roundnode]    (x1)                 {$x_1$};
                \node[roundnode]    (x2)   [below=of x1] {$x_2$};
                
                \node[dot]          (d1)   [below=of x2] {};
                \node[dot]          (d2)   [below=of d1] {};
                \node[dot]          (d3)   [below=of d2] {};
                
                \node[roundnode]    (xn)   [below=of d3] {$x_n$};
                
                
                
                \node[roundnode1]    (n1)   [right=of x1, xshift= 1.5cm, yshift = -1cm] {$n_1$};
                \node[roundnode1]    (n2)   [below=of n1] {$n_2$};
                \node[roundnode1]    (n3)   [below=of n2] {$n_3$};
                
                \node[roundnode1]    (n5)   [right=of n2, yshift= - 1.75cm] {$n_5$};
                \node[roundnode1]    (n4)   [above=of n5] {$n_4$};

                \node[roundnode1]    (n6)   [right=of n4, yshift= - 1.7cm] {$n_6$};

                \node[roundnode2]    (n7)   [right=of n6] {$y$};
            
                %Lines
                \draw[-biggertip] (x1.east) -- (n1.west);
                \draw[-biggertip] (x2.east) -- (n1.west);
                \draw[-biggertip] (xn.east) -- (n1.west);

                \draw[-biggertip] (x1.east) -- (n2.west);
                \draw[-biggertip] (x2.east) -- (n2.west);
                \draw[-biggertip] (xn.east) -- (n2.west);

                \draw[-biggertip] (x1.east) -- (n3.west);
                \draw[-biggertip] (x2.east) -- (n3.west);
                \draw[-biggertip] (xn.east) -- (n3.west);


                \draw[-biggertip] (n1.east) -- (n4.west);
                \draw[-biggertip] (n2.east) -- (n4.west);
                \draw[-biggertip] (n3.east) -- (n4.west);

                \draw[-biggertip] (n1.east) -- (n5.west);
                \draw[-biggertip] (n2.east) -- (n5.west);
                \draw[-biggertip] (n3.east) -- (n5.west);
                

                \draw[-biggertip] (n4.east) -- (n6.west);
                \draw[-biggertip] (n5.east) -- (n6.west);


                \draw[-biggertip] (n6.east) -- (n7.west);


            \end{tikzpicture}
            }
            \caption{Схема нейронної мережі з трьома шарами}
    \end{center}
\end{figure}

Перед початком навчання ваги та зсуви ініціалізуються випадковими значеннями. 
Це допомагає уникнути симетрії між нейронами і забезпечує різні шляхи навчання для кожного з них. 
Далі йде розрахунок кінцевого значення моделі, яке порівнюється з реальним значенням за допомогою функції втрат. 
Функції витрат визначають, наскільки відрізняються прогнозовані значення від реальних. 
Після цього відбувається корекції ваг та зсувів. 
Якщо розгладати функцію витрат, як багатовимірну функцію від ваг, то корекція полягає у застосування градієнтного методу оптимізації. 
Градієнт розграховується в оберненому порядку від обчислення результату моделі, тобто спочатку оптимізуються ваги останнього шару, потім передостаннього і так далі. 
Функції активації підібрані так, що градієнт можна обчислити аналітично. 
Цей процес повторюється для кожного набору даних, поки результат не задовільнить визначені критерії визначені для моделі. 

Для нашого випадку велика нейронна мережа з багатьма шарами не підійде, адже під час навчання мадель запам'ятає дані і не зважаючи на те, що результат на навчальних даних буде дуже точним, на тестових даних результат буде поганим. 
У ході експериментів з різними розмірами моделі було виявлено, що найкращі результати показує модель з чотирма шарами. 
Перші два шари по десять нейронів, третій - вісім, а останній - сім. 
В останньому шарі кожен нейрон відповідає за прогнозування одного типу викидів. 

\subsection{Результати}

Дані було нормовано перед введенням в модель, щоб уникнути проблеми з великими значеннями ваг. 
Нейронні мережі не потребують нормального розподілу даних проте задля їх використання дані мають мати сильну залежність від параметрів інакше вона буде підлаштовуватись під шум. 



\begin{center}
    \scalebox{1}{
        \begin{tabular}{ |p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
            \hline
            \multicolumn{3}{|c|}{Середня похибка} \\
            \hline
            Викиди & модель & наївний алг \\
            \hline
            \multicolumn{3}{|c|}{Тренувальні дані} \\
            \hline
            CO2  &   1.43 \% &   6.29 \% \\
            \hline
            pm10  &  48.00 \% &   15.08 \% \\
            \hline
            pm2.5  &   4.63 \% &   18 \% \\
            \hline
            озон  &   1.69 \% &   3.72 \% \\
            \hline
            діоксид азоту  &   8.02 \% &  13.23 \% \\
            \hline
            діоксид сірки  &   6.74 \% &  10.78 \% \\
            \hline
            оксид азоту  &   9.37 \% &  15.43 \% \\
            \hline
            \multicolumn{3}{|c|}{Тестові дані} \\
            \hline
            CO2  &  11.02 \% &  20.4 \% \\
            \hline
            pm10  &  59.66 \% &  19.76 \% \\
            \hline
            pm2.5  &  13.98 \% &   14.55 \% \\
            \hline
            озон  &   5.19 \% &   12.04 \% \\
            \hline
            діоксид азоту  &  39.80 \% &  24.93 \% \\
            \hline
            діоксид сірки  &  38.06 \% &  32.79 \% \\
            \hline
            оксид азоту  &  47.09 \% &  39.29 \% \\
            \hline
        \end{tabular}
    }
    \vspace{1cm}

    \labelformat{2}{}{Таблиця 3.2: Результати нейронної мережі в порівнянні з наївним алгоритмом}
\end{center}

Результати роботи нейронної мережі на тренувальних даних показують, що модель демонструє високу точність для прогнозування рівня вуглекислого газу, озону, PM2.5 та інших параметрів, що свідчить про здатність моделі ефективно враховувати складні взаємозв'язки в даних. 
Наприклад, для вуглекислого газу та озону середні похибки на тренувальних даних є відносно низькими.

Тестові дані виявляють декілька проблем в прогнозуваннях.
Для вуглекислого газу та озону наша модель має помітно нижчу похибку порівняно з наївним алгоритмом та лінійною регресією, що свідчить про кращу здатність моделі враховувати залежності в даних. 
Однак для таких параметрів, як PM10 та оксидів, результати показують, що прогнозовані дані слабко залежать від параметрів прогнозування, і точний прогноз не можливий без виявлення інших параметрів, що дійсно впливають на даний вид викиду. 
Зважаючи на похибки на тренуванні можна зробити висновок, що модель підлаштувалась під шум на тренувальних данних і саме тому результат так сильно відрізняється.
Це може свідчити про наявність неврахованих факторів або випадкових коливань у даних.

